{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7ee849c",
   "metadata": {},
   "source": [
    "## 1.0 SETUP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4e75ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy and pandas libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab36832d",
   "metadata": {},
   "source": [
    "## 2.0 Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d24ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"./train_X.csv\")\n",
    "X_test = pd.read_csv(\"./test_X.csv\")\n",
    "y_train = pd.read_csv(\"./train_y.csv\")\n",
    "y_test = pd.read_csv(\"./test_y.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16aeebf",
   "metadata": {},
   "source": [
    "### For predicting whether an NBA player will play for 5 years or not, the more relevant metric to use would be Precision.\n",
    "\n",
    "**Precision** is the proportion of true positives out of all the predicted positives. \n",
    "In our case, a forecast that a player will play for at least five years would be considered a true positive, whereas a prediction that a player will play for at least five years but eventually retire or sustain a career-ending injury would be considered a false positive.\n",
    "\n",
    "The reason I chose precision and why precision is more relevant in this case is that it is more important to avoid false positives (i.e., predicting a player will play for 5 years when they won't) than to avoid false negatives (i.e., predicting a player won't play for 5 years when they will).\n",
    "\n",
    "I went with **precision** since it's more essential to prevent false positives (for example, forecasting a player will play for 5 years when they won't) than false negatives (for example, predicting a player won't play for 5 years when they will) in this situation.\n",
    "\n",
    "This is because false positives can lead to significant financial losses for NBA teams if they invest in players who retire early or suffer career-ending injuries, while investing in players who are predicted to play for less than 5 years, but end up playing for longer is not a financial loss for the teams.\n",
    "\n",
    "Therefore, precision is the more relevant metric to use when evaluating the performance of a predictive model for whether NBA players will play for 5 years or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea93f95d",
   "metadata": {},
   "source": [
    "## 3.0 Model the data\n",
    "First, we will create a dataframe to hold all the results of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4cdf6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame({\"model\": [], \"Accuracy\": [], \"Precision\": [], \"Recall\": [], \"F1\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d31ae5",
   "metadata": {},
   "source": [
    "## 3.1.1 Logistic regression using random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72350f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n",
      "The best precision score is 0.7033599200498099\n",
      "... with parameters: {'solver': 'liblinear', 'penalty': 'l2', 'max_iter': 601}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "1255 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "645 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 441, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got None.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "340 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1471, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "270 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 457, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.69690454        nan        nan        nan\n",
      " 0.69690454 0.70335992 0.70335992        nan        nan        nan\n",
      " 0.69690454        nan 0.69690454 0.69690454 0.70335992        nan\n",
      "        nan        nan        nan 0.69690454 0.70335992 0.70335992\n",
      " 0.69690454        nan        nan        nan 0.70335992        nan\n",
      " 0.70335992 0.70335992 0.70335992        nan        nan        nan\n",
      "        nan        nan 0.69690454 0.70335992 0.70335992        nan\n",
      " 0.70335992        nan        nan        nan        nan        nan\n",
      " 0.69690454 0.69690454        nan        nan        nan        nan\n",
      " 0.69690454 0.69690454 0.70335992 0.70335992        nan 0.70335992\n",
      " 0.70335992        nan 0.69690454        nan        nan 0.69690454\n",
      " 0.70335992 0.69690454 0.70335992 0.69690454        nan        nan\n",
      " 0.70335992 0.70335992 0.69690454 0.69690454        nan        nan\n",
      " 0.70335992 0.70335992 0.70335992 0.69690454        nan        nan\n",
      " 0.69690454 0.69690454        nan 0.69690454 0.69690454 0.69690454\n",
      " 0.69690454 0.69690454 0.69690454 0.70335992 0.69690454 0.69690454\n",
      " 0.70335992        nan 0.69690454 0.69690454        nan        nan\n",
      "        nan 0.70335992 0.70335992 0.70335992        nan 0.70335992\n",
      " 0.70335992 0.69690454        nan        nan 0.69690454 0.69690454\n",
      " 0.69690454        nan 0.69690454 0.70335992        nan        nan\n",
      "        nan 0.70335992 0.70335992        nan        nan 0.70335992\n",
      " 0.69690454 0.69690454        nan 0.70335992 0.69690454        nan\n",
      "        nan 0.70335992 0.69690454        nan        nan 0.69690454\n",
      " 0.70335992        nan        nan        nan 0.69690454        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.69690454        nan 0.70335992        nan 0.70335992        nan\n",
      "        nan 0.69690454        nan        nan 0.70335992        nan\n",
      " 0.69690454 0.69690454        nan        nan        nan 0.70335992\n",
      "        nan        nan 0.70335992        nan        nan 0.69690454\n",
      " 0.70335992 0.69690454        nan 0.70335992 0.69690454 0.70335992\n",
      "        nan 0.69690454        nan 0.69690454        nan        nan\n",
      "        nan 0.70335992        nan        nan 0.70335992 0.69690454\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.69690454 0.70335992        nan 0.69690454\n",
      "        nan 0.69690454 0.70335992        nan 0.70335992 0.69690454\n",
      " 0.69690454 0.70335992 0.70335992        nan 0.69690454        nan\n",
      "        nan        nan 0.70335992 0.69690454 0.70335992 0.70335992\n",
      " 0.70335992        nan        nan 0.70335992        nan        nan\n",
      "        nan 0.69690454        nan        nan        nan 0.70335992\n",
      "        nan 0.70335992 0.69690454        nan 0.70335992 0.70335992\n",
      "        nan        nan        nan        nan 0.70335992 0.70335992\n",
      " 0.69690454 0.70335992        nan 0.69690454 0.69690454 0.69690454\n",
      "        nan 0.70335992 0.70335992 0.69690454        nan 0.70335992\n",
      " 0.70335992 0.69690454 0.70335992        nan        nan        nan\n",
      "        nan 0.69690454        nan        nan        nan        nan\n",
      " 0.70335992        nan        nan        nan        nan 0.70335992\n",
      "        nan 0.70335992        nan        nan 0.69690454        nan\n",
      " 0.70335992        nan 0.69690454 0.70335992        nan        nan\n",
      " 0.70335992 0.70335992 0.70335992        nan 0.69690454 0.69690454\n",
      " 0.70335992        nan 0.69690454 0.69690454        nan 0.70335992\n",
      "        nan        nan 0.69690454 0.69690454        nan        nan\n",
      " 0.69690454 0.70335992        nan 0.69690454        nan        nan\n",
      "        nan        nan 0.70335992 0.69690454        nan        nan\n",
      "        nan 0.69690454        nan        nan 0.69690454 0.69690454\n",
      "        nan        nan 0.69690454        nan 0.69690454        nan\n",
      " 0.69690454        nan        nan 0.69690454 0.69690454 0.70335992\n",
      "        nan        nan        nan 0.70335992 0.69690454        nan\n",
      "        nan 0.70335992 0.69690454        nan 0.69690454        nan\n",
      "        nan        nan        nan 0.70335992        nan 0.69690454\n",
      "        nan 0.70335992        nan        nan        nan        nan\n",
      " 0.70335992        nan        nan 0.69690454        nan 0.70335992\n",
      " 0.70335992        nan        nan        nan 0.69690454 0.69690454\n",
      " 0.69690454 0.70335992        nan        nan        nan        nan\n",
      " 0.69690454 0.70335992        nan        nan 0.69690454 0.69690454\n",
      "        nan 0.69690454        nan        nan 0.70335992 0.69690454\n",
      "        nan        nan        nan 0.70335992        nan 0.69690454\n",
      " 0.70335992        nan 0.70335992 0.69690454        nan 0.70335992\n",
      "        nan 0.69690454        nan 0.70335992        nan 0.69690454\n",
      "        nan 0.70335992 0.69690454 0.69690454        nan 0.69690454\n",
      " 0.69690454        nan        nan        nan 0.69690454 0.70335992\n",
      "        nan 0.69690454 0.69690454 0.70335992 0.69690454 0.70335992\n",
      " 0.70335992        nan 0.69690454 0.69690454 0.69690454 0.70335992\n",
      "        nan        nan        nan        nan 0.69690454        nan\n",
      "        nan 0.69690454 0.70335992 0.69690454        nan 0.70335992\n",
      "        nan 0.70335992        nan        nan        nan 0.69690454\n",
      " 0.70335992 0.70335992        nan 0.70335992        nan 0.69690454\n",
      "        nan        nan        nan 0.69690454 0.70335992 0.70335992\n",
      "        nan        nan        nan 0.69690454        nan 0.69690454\n",
      "        nan        nan 0.70335992 0.69690454 0.70335992 0.69690454\n",
      " 0.69690454        nan        nan        nan        nan 0.69690454\n",
      " 0.69690454 0.69690454 0.69690454        nan        nan        nan\n",
      "        nan        nan 0.70335992        nan        nan        nan\n",
      " 0.69690454        nan        nan 0.69690454 0.69690454        nan\n",
      " 0.69690454 0.70335992]\n",
      "  warnings.warn(\n",
      "C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [       nan        nan 0.7133131         nan        nan        nan\n",
      " 0.7133131  0.71998661 0.71998661        nan        nan        nan\n",
      " 0.7133131         nan 0.71573579 0.7133131  0.71998661        nan\n",
      "        nan        nan        nan 0.7133131  0.71998661 0.71998661\n",
      " 0.71573579        nan        nan        nan 0.71998661        nan\n",
      " 0.71998661 0.71998661 0.71998661        nan        nan        nan\n",
      "        nan        nan 0.7133131  0.71998661 0.71998661        nan\n",
      " 0.71998661        nan        nan        nan        nan        nan\n",
      " 0.7133131  0.71573579        nan        nan        nan        nan\n",
      " 0.71573579 0.71573579 0.71998661 0.71998661        nan 0.71998661\n",
      " 0.71998661        nan 0.71573579        nan        nan 0.71573579\n",
      " 0.71998661 0.7133131  0.71998661 0.71573579        nan        nan\n",
      " 0.71998661 0.71998661 0.7133131  0.71573579        nan        nan\n",
      " 0.71998661 0.71998661 0.71998661 0.71573579        nan        nan\n",
      " 0.7133131  0.7133131         nan 0.7133131  0.71573579 0.7133131\n",
      " 0.71573579 0.71573579 0.7133131  0.71998661 0.71573579 0.7133131\n",
      " 0.71998661        nan 0.7133131  0.7133131         nan        nan\n",
      "        nan 0.71998661 0.71998661 0.71998661        nan 0.71998661\n",
      " 0.71998661 0.71573579        nan        nan 0.71573579 0.71573579\n",
      " 0.71573579        nan 0.71573579 0.71998661        nan        nan\n",
      "        nan 0.71998661 0.71998661        nan        nan 0.71998661\n",
      " 0.71573579 0.7133131         nan 0.71998661 0.71573579        nan\n",
      "        nan 0.71998661 0.71573579        nan        nan 0.7133131\n",
      " 0.71998661        nan        nan        nan 0.7133131         nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.71573579        nan 0.71998661        nan 0.71998661        nan\n",
      "        nan 0.71573579        nan        nan 0.71998661        nan\n",
      " 0.7133131  0.7133131         nan        nan        nan 0.71998661\n",
      "        nan        nan 0.71998661        nan        nan 0.7133131\n",
      " 0.71998661 0.7133131         nan 0.71998661 0.71573579 0.71998661\n",
      "        nan 0.71573579        nan 0.71573579        nan        nan\n",
      "        nan 0.71998661        nan        nan 0.71998661 0.71573579\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.71573579 0.71998661        nan 0.71573579\n",
      "        nan 0.71573579 0.71998661        nan 0.71998661 0.71573579\n",
      " 0.7133131  0.71998661 0.71998661        nan 0.7133131         nan\n",
      "        nan        nan 0.71998661 0.7133131  0.71998661 0.71998661\n",
      " 0.71998661        nan        nan 0.71998661        nan        nan\n",
      "        nan 0.71573579        nan        nan        nan 0.71998661\n",
      "        nan 0.71998661 0.71573579        nan 0.71998661 0.71998661\n",
      "        nan        nan        nan        nan 0.71998661 0.71998661\n",
      " 0.71573579 0.71998661        nan 0.71573579 0.71573579 0.71573579\n",
      "        nan 0.71998661 0.71998661 0.71573579        nan 0.71998661\n",
      " 0.71998661 0.7133131  0.71998661        nan        nan        nan\n",
      "        nan 0.71573579        nan        nan        nan        nan\n",
      " 0.71998661        nan        nan        nan        nan 0.71998661\n",
      "        nan 0.71998661        nan        nan 0.71573579        nan\n",
      " 0.71998661        nan 0.71573579 0.71998661        nan        nan\n",
      " 0.71998661 0.71998661 0.71998661        nan 0.71573579 0.7133131\n",
      " 0.71998661        nan 0.7133131  0.71573579        nan 0.71998661\n",
      "        nan        nan 0.71573579 0.71573579        nan        nan\n",
      " 0.71573579 0.71998661        nan 0.7133131         nan        nan\n",
      "        nan        nan 0.71998661 0.71573579        nan        nan\n",
      "        nan 0.7133131         nan        nan 0.71573579 0.71573579\n",
      "        nan        nan 0.71573579        nan 0.7133131         nan\n",
      " 0.7133131         nan        nan 0.7133131  0.7133131  0.71998661\n",
      "        nan        nan        nan 0.71998661 0.71573579        nan\n",
      "        nan 0.71998661 0.7133131         nan 0.71573579        nan\n",
      "        nan        nan        nan 0.71998661        nan 0.7133131\n",
      "        nan 0.71998661        nan        nan        nan        nan\n",
      " 0.71998661        nan        nan 0.7133131         nan 0.71998661\n",
      " 0.71998661        nan        nan        nan 0.7133131  0.7133131\n",
      " 0.7133131  0.71998661        nan        nan        nan        nan\n",
      " 0.7133131  0.71998661        nan        nan 0.71573579 0.71573579\n",
      "        nan 0.71573579        nan        nan 0.71998661 0.7133131\n",
      "        nan        nan        nan 0.71998661        nan 0.71573579\n",
      " 0.71998661        nan 0.71998661 0.7133131         nan 0.71998661\n",
      "        nan 0.71573579        nan 0.71998661        nan 0.71573579\n",
      "        nan 0.71998661 0.7133131  0.7133131         nan 0.71573579\n",
      " 0.7133131         nan        nan        nan 0.71573579 0.71998661\n",
      "        nan 0.7133131  0.71573579 0.71998661 0.7133131  0.71998661\n",
      " 0.71998661        nan 0.71573579 0.7133131  0.71573579 0.71998661\n",
      "        nan        nan        nan        nan 0.7133131         nan\n",
      "        nan 0.71573579 0.71998661 0.71573579        nan 0.71998661\n",
      "        nan 0.71998661        nan        nan        nan 0.71573579\n",
      " 0.71998661 0.71998661        nan 0.71998661        nan 0.71573579\n",
      "        nan        nan        nan 0.7133131  0.71998661 0.71998661\n",
      "        nan        nan        nan 0.7133131         nan 0.71479801\n",
      "        nan        nan 0.71998661 0.7133131  0.71998661 0.7133131\n",
      " 0.71573579        nan        nan        nan        nan 0.71573579\n",
      " 0.7133131  0.7133131  0.7133131         nan        nan        nan\n",
      "        nan        nan 0.71998661        nan        nan        nan\n",
      " 0.71573579        nan        nan 0.7133131  0.7133131         nan\n",
      " 0.7133131  0.71998661]\n",
      "  warnings.warn(\n",
      "C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'max_iter':np.arange(500,1000),\n",
    "    'penalty': ['None','l1','l2','elasticnet'],\n",
    "    'solver':['saga','liblinear']\n",
    "}\n",
    "\n",
    "log_reg_model = LogisticRegression()\n",
    "rand_search = RandomizedSearchCV(estimator = log_reg_model, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1, \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93527f0",
   "metadata": {},
   "source": [
    "## 3.1.2 Logistic regression using grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a84387e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "The best precision score is 0.7033599200498099\n",
      "... with parameters: {'max_iter': 596, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "max_iter = rand_search.best_params_['max_iter']\n",
    "penalty = rand_search.best_params_['penalty']\n",
    "solver = rand_search.best_params_['solver']\n",
    "\n",
    "param_grid = {\n",
    "    'max_iter': np.arange(max_iter-5,max_iter+5),  \n",
    "    'penalty': [penalty],\n",
    "    'solver': [solver]\n",
    "}\n",
    "\n",
    "logistic_reg_model = LogisticRegression()\n",
    "grid_search = GridSearchCV(estimator = logistic_reg_model, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallLogistic = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02da1d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Logistic Regression\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])\n",
    "log_reg_bm=grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e1989",
   "metadata": {},
   "source": [
    "## 3.2.1 Decision Tree using random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aa63225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 70 candidates, totalling 350 fits\n",
      "The best precision score is 0.7422923937663772\n",
      "... with parameters: {'min_samples_split': 4, 'min_samples_leaf': 5, 'min_impurity_decrease': 0.0001, 'max_leaf_nodes': 58, 'max_depth': 16, 'criterion': 'gini'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 350.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.67788661 0.6890523  0.67132506 0.70642164 0.70315638 0.70444991\n",
      " 0.69873944 0.69760937 0.6986813  0.67853129 0.68411707 0.70618003\n",
      " 0.74229239 0.6890523  0.70014554 0.67668404 0.69873944 0.69574199\n",
      " 0.69939779 0.70236348 0.68945898 0.67853129        nan 0.69108872\n",
      " 0.70113773 0.71763915 0.71299015 0.68411707 0.69517483 0.68796759\n",
      " 0.7006739  0.69184352 0.69103353 0.69873944 0.70404969 0.67206007\n",
      " 0.70113773 0.70573199 0.71974937 0.69939779 0.67242924 0.69973193\n",
      " 0.71974937 0.66771833 0.67668404 0.6870828  0.69154636 0.68106644\n",
      " 0.71389988 0.69552985 0.70404969 0.70954763 0.69966817 0.69904766\n",
      " 0.67878189 0.69450314 0.71406818 0.70404969 0.70113773 0.67838787\n",
      " 0.69809712 0.70236348 0.69873944 0.71307643 0.67853129 0.68659587\n",
      " 0.70236624 0.70033208 0.69856668 0.70053053]\n",
      "  warnings.warn(\n",
      "C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.72709878 0.72041431 0.71003593 0.78515124 0.74562536 0.77869546\n",
      " 0.71191914 0.71921824 0.74131058 0.68675689 0.71782609 0.7443632\n",
      " 0.86932158 0.72041431 0.74596905 0.72538562 0.71191914 0.7275271\n",
      " 0.72998212 0.72720194 0.73878413 0.68675689        nan 0.70767497\n",
      " 0.72980627 0.7361229  0.74823078 0.71782609 0.72132719 0.71677468\n",
      " 0.79529086 0.74013954 0.73087374 0.71191914 0.72640111 0.71925649\n",
      " 0.72980627 0.80676946 0.74440097 0.72998212 0.7235371  0.72706827\n",
      " 0.74440097 0.70365512 0.72538562 0.74403156 0.75078911 0.7219833\n",
      " 0.77153848 0.76388846 0.72640111 0.74777155 0.73216278 0.78285969\n",
      " 0.72726044 0.75036675 0.73475471 0.72640111 0.72980627 0.72728205\n",
      " 0.75337306 0.72720194 0.71191914 0.7341439  0.68675689 0.73191862\n",
      " 0.73223161 0.71229231 0.71993164 0.74041331]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(1,100),  \n",
    "    'min_samples_leaf': np.arange(1,100),\n",
    "    'min_impurity_decrease': np.arange(0.0001, 0.01, 0.0005),\n",
    "    'max_leaf_nodes': np.arange(5, 100), \n",
    "    'max_depth': np.arange(1,20), \n",
    "    'criterion': ['entropy', 'gini'],\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "rand_search = RandomizedSearchCV(estimator = dtree, param_distributions=param_grid, cv=kfolds, n_iter=70,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,  # n_jobs=-1 will utilize all available CPUs \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = rand_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872cf8d5",
   "metadata": {},
   "source": [
    "## 3.2.2 Decision Tree using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b71d811a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1024 candidates, totalling 5120 fits\n",
      "The best precision score is 0.7586689011760931\n",
      "... with parameters: {'criterion': 'gini', 'max_depth': 17, 'max_leaf_nodes': 57, 'min_impurity_decrease': 5e-05, 'min_samples_leaf': 3, 'min_samples_split': 3}\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "min_samples_split = rand_search.best_params_['min_samples_split']\n",
    "min_samples_leaf = rand_search.best_params_['min_samples_leaf']\n",
    "min_impurity_decrease = rand_search.best_params_['min_impurity_decrease']\n",
    "max_leaf_nodes = rand_search.best_params_['max_leaf_nodes']\n",
    "max_depth = rand_search.best_params_['max_depth']\n",
    "criterion = rand_search.best_params_['criterion']\n",
    "\n",
    "param_grid = {\n",
    "    'min_samples_split': np.arange(min_samples_split-2,min_samples_split+2),  \n",
    "    'min_samples_leaf': np.arange(min_samples_leaf-2,min_samples_leaf+2),\n",
    "    'min_impurity_decrease': np.arange(min_impurity_decrease-0.0001, min_impurity_decrease+0.0001, 0.00005),\n",
    "    'max_leaf_nodes': np.arange(max_leaf_nodes-2,max_leaf_nodes+2), \n",
    "    'max_depth': np.arange(max_depth-2,max_depth+2), \n",
    "    'criterion': [criterion]\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier()\n",
    "grid_search = GridSearchCV(estimator = dtree, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallTree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6776b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"Decision Tree\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63910822",
   "metadata": {},
   "source": [
    "## 3.3.1 SVM using random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d51c9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:292: UserWarning: The total space of parameters 144 is smaller than n_iter=500. Running 144 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best precision score is 0.7779774253075223\n",
      "... with parameters: {'kernel': 'rbf', 'gamma': 'auto', 'C': 15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "param_grid = {\n",
    "    'C': np.arange(1,25),   \n",
    "    'gamma': ['scale','auto'],\n",
    "    'kernel':['linear','rbf','poly']\n",
    "}\n",
    "\n",
    "svm_model = SVC()\n",
    "rand_search = RandomizedSearchCV(estimator = svm_model, param_distributions=param_grid, cv=kfolds, n_iter=500,\n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1, \n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = rand_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {rand_search.best_score_}\")\n",
    "print(f\"... with parameters: {rand_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e017815a",
   "metadata": {},
   "source": [
    "## 3.3.2 SVM using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1ef34b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "The best precision score is 0.7779774253075223\n",
      "... with parameters: {'C': 15, 'gamma': 'auto', 'kernel': 'rbf'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risha\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "score_measure = \"precision\"\n",
    "kfolds = 5\n",
    "\n",
    "C = rand_search.best_params_['C']\n",
    "gamma = rand_search.best_params_['gamma']\n",
    "kernel = rand_search.best_params_['kernel']\n",
    "\n",
    "param_grid = {\n",
    "    'C': np.arange(C-2,C+2),  \n",
    "    'gamma': [gamma],\n",
    "    'kernel': [kernel]\n",
    "    \n",
    "}\n",
    "\n",
    "svm_model = SVC()\n",
    "grid_search = GridSearchCV(estimator = svm_model, param_grid=param_grid, cv=kfolds, \n",
    "                           scoring=score_measure, verbose=1, n_jobs=-1,\n",
    "                           return_train_score=True)\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"The best {score_measure} score is {grid_search.best_score_}\")\n",
    "print(f\"... with parameters: {grid_search.best_params_}\")\n",
    "\n",
    "bestRecallSVM = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34e29b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_matrix = confusion_matrix(y_test, grid_search.predict(X_test))\n",
    "TP = c_matrix[1][1]\n",
    "TN = c_matrix[0][0]\n",
    "FP = c_matrix[0][1]\n",
    "FN = c_matrix[1][0]\n",
    "performance = pd.concat([performance, pd.DataFrame({'model':\"SVM\", \n",
    "                                                    'Accuracy': [(TP+TN)/(TP+TN+FP+FN)], \n",
    "                                                    'Precision': [TP/(TP+FP)], \n",
    "                                                    'Recall': [TP/(TP+FN)], \n",
    "                                                    'F1': [2*TP/(2*TP+FP+FN)]\n",
    "                                                     }, index=[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783e45ec",
   "metadata": {},
   "source": [
    "## 3.4 Evaluating the performance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3069c5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.669154</td>\n",
       "      <td>0.778325</td>\n",
       "      <td>0.642276</td>\n",
       "      <td>0.703786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.651741</td>\n",
       "      <td>0.726496</td>\n",
       "      <td>0.691057</td>\n",
       "      <td>0.708333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.619403</td>\n",
       "      <td>0.718310</td>\n",
       "      <td>0.621951</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  Accuracy  Precision    Recall        F1\n",
       "0  Logistic Regression  0.669154   0.778325  0.642276  0.703786\n",
       "0        Decision Tree  0.651741   0.726496  0.691057  0.708333\n",
       "0                  SVM  0.619403   0.718310  0.621951  0.666667"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b0cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving our best model for future use cases\n",
    "#here the best model is logistic regression\n",
    "BestModel=log_reg_bm\n",
    "pickle.dump(best_model, open('LogisticRegression.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e0fee",
   "metadata": {},
   "source": [
    "## 4.0 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e355e236",
   "metadata": {},
   "source": [
    "Prediction of career length of an NBA player is very important for NBA teams as an effective predictive model would help the teams to take more informed decisions on the signings of players. This would help the teams to spend their limited resources and money on players who have a higher career length and can be more impactful for the team for a longer period of time and ultimately being a profitable deal. \n",
    "\n",
    "Out of all the models, the best performing model based on the metric *Precision* is **Logistic Regression**. It has a precision value of **0.778**. It also has highest accuracy **(0.669)** and good recall **(0.642)** and F1 score **(0.703)** as well. This means that our best performing model can correctly identify 77.8% players that whether they will play for 5 years or not. \n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a0b24c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
